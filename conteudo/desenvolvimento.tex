\chapter{Desenvolvimento} \label{chap:desenvolvimento}

Neste capítulo, detalhou-se o processo de desenvolvimento do sistema de detecção automática de anomalias visuais em jogos digitais utilizando Redes Neurais Convolucionais (CNNs). O desenvolvimento foi estruturado em várias etapas, desde a preparação dos dados até a implementação e avaliação do modelo.

\section{Metodologia Aplicada}

O trabalho adotou uma abordagem quantitativa, aplicada e experimental, com o objetivo de investigar o uso de CNNs para a detecção automática de anomalias visuais em jogos digitais. A pesquisa fundamentou-se na análise sistemática de dados visuais para mensurar o desempenho do modelo na identificação de falhas gráficas, tais como objetos flutuantes, texturas corrompidas e glitches visuais.

A metodologia aplicada foi estruturada em cinco etapas principais, descritas a seguir.

\subsection{Base de Dados}
Foram utilizadas imagens de jogos digitais provenientes do dataset \textit{Gameplay Images}, disponível no repositório Kaggle (\citeonline{KaggleGameplayImages}). O conjunto continha aproximadamente \textbf{10\,000 imagens} no formato \texttt{.png}, extraídas de \textit{frames} de vídeos do YouTube e organizadas em \textbf{10 classes}, correspondentes a jogos populares: \textit{Among Us}, \textit{Apex Legends}, \textit{Fortnite}, \textit{Forza Horizon}, \textit{Free Fire}, \textit{Genshin Impact}, \textit{God of War}, \textit{Minecraft}, \textit{Roblox} e \textit{Terraria}. Cada classe continha cerca de 1\,000 imagens com resolução padronizada de 640×360 pixels.

Para a criação de classes com anomalias, foram aplicadas artificialmente distorções visuais às imagens originais por meio de bibliotecas de processamento de imagem, como \texttt{imgaug} e \texttt{albumentations}, simulando falhas gráficas comuns. Entre as distorções aplicadas estavam: \textit{blur} (desfoque), \textit{noise} (ruído), \textit{pixel dropout}, distorções geométricas e alterações abruptas de cor.

Dessa forma, o conjunto de dados foi composto por imagens “normais” e imagens “com anomalias”, garantindo controle experimental sobre os tipos de falhas presentes e permitindo a avaliação precisa do modelo.

\subsection{Pré-processamento}
Todas as imagens foram redimensionadas e normalizadas para garantir uniformidade no processamento, o que foi fundamental para a eficiência do treinamento da rede neural.

\subsection{Anotação das Anomalias}
As imagens tiveram seus rótulos atribuídos automaticamente, conforme o processo de aplicação das anomalias (original = normal; distorcida = com anomalias), configurando assim um conjunto rotulado para aprendizado supervisionado.

\subsection{Aumento da Base de Dados (Data Augmentation)}
Foram aplicadas técnicas como rotações, espelhamentos e variações de brilho para ampliar a variabilidade dos dados de entrada, fortalecendo a robustez do modelo e evitando \textit{overfitting}.

\subsection{Implementação da CNN}
O modelo foi desenvolvido utilizando bibliotecas como TensorFlow e PyTorch, com foco na classificação binária. O treinamento foi realizado com os dados rotulados e o desempenho avaliado por meio de um conjunto de teste, utilizando métricas quantitativas como acurácia, precisão, recall e matriz de confusão. O tempo de inferência foi considerado como métrica adicional de eficiência.

\section{Arquitetura do Modelo}

A arquitetura proposta para o sistema de detecção automática de anomalias visuais em jogos digitais foi organizada em duas etapas principais: (i) geração e preparação das imagens com anomalias e (ii) treinamento da Rede Neural Convolucional (CNN) responsável pela classificação binária (normal/anômala).

\subsection{Geração de Anomalias e Pré-processamento}

Antes do treinamento da rede, foi desenvolvido um módulo em \texttt{Python} para a criação automática de anomalias visuais sintéticas a partir de imagens originais de gameplay. Esse módulo foi implementado utilizando as bibliotecas \texttt{OpenCV}, \texttt{NumPy}, \texttt{Matplotlib} e \texttt{Albumentations}, além de transformações customizadas definidas manualmente.

Foram criadas seis funções de distorção visual com o objetivo de simular falhas gráficas reais em jogos digitais, incluindo texturas ausentes, ruídos visuais e artefatos de renderização. As transformações desenvolvidas foram:

\begin{itemize}
    \item \textbf{Missing Texture}: substitui regiões da imagem por blocos coloridos ou padrões quadriculados, simulando falhas no carregamento de texturas;
    \item \textbf{Screen Tearing}: desloca faixas horizontais da imagem, reproduzindo o efeito de rasgo na tela comum em renderizações instáveis;
    \item \textbf{UI Occlusion}: insere retângulos coloridos aleatórios, representando elementos de interface sobrepostos de forma incorreta;
    \item \textbf{Pixelation}: reduz a resolução local de determinadas áreas, criando efeito de pixelização forçada;
    \item \textbf{Polygon Clipping}: aplica máscaras poligonais aleatórias, gerando cortes e áreas com preenchimento incorreto;
    \item \textbf{Shader Artifacts}: altera a luminosidade por faixas verticais, simulando falhas em shaders e iluminação.
\end{itemize}

Essas transformações foram agrupadas por meio da classe \texttt{A.Compose()} da biblioteca \texttt{Albumentations}, permitindo a aplicação sequencial de múltiplos efeitos sobre cada imagem de entrada. O pipeline foi configurado para gerar três variações anômalas por imagem original, ampliando o conjunto de treinamento e garantindo maior diversidade visual.

Todas as imagens foram redimensionadas para $640\times360$ pixels e normalizadas, garantindo consistência durante o processamento. O sistema foi projetado para exibir visualmente amostras de imagens originais e distorcidas, possibilitando a verificação qualitativa dos artefatos gerados.

\subsection{Arquitetura da CNN}

A rede neural convolucional foi implementada utilizando as bibliotecas \texttt{TensorFlow} e \texttt{PyTorch}, com foco na classificação binária entre imagens normais e anômalas. A arquitetura foi composta por múltiplas camadas convolucionais intercaladas com funções de ativação \texttt{ReLU}, camadas de \texttt{MaxPooling} e blocos de normalização por lotes (\texttt{Batch Normalization}). Ao final, foram adicionadas camadas densas (\texttt{Fully Connected}) e uma camada de saída com ativação \texttt{Sigmoid}, adequada para problemas binários.

O treinamento utilizou a função de perda \texttt{Binary Cross-Entropy} e o otimizador \texttt{Adam}, com taxa de aprendizado ajustada empiricamente. O processo foi monitorado por meio das métricas de acurácia, precisão, \textit{recall} e análise da matriz de confusão, além da medição do tempo médio de inferência, que serviu como indicador de eficiência computacional.

\subsection{Experimentos Realizados}

Após a definição da arquitetura e do processo de treinamento, foram conduzidos três experimentos principais, cada um com objetivos específicos. Esses experimentos tiveram como propósito avaliar diferentes aspectos do modelo, desde a escolha da arquitetura até estratégias de otimização e generalização. A seguir, descrevem-se os experimentos realizados:

\begin{itemize}
    \item \textbf{Experimento 1 – Comparação de Arquiteturas CNN}: comparação do desempenho de diferentes arquiteturas de redes convolucionais (VGG16, VGG19, ResNet50, DenseNet121, EfficientNetV2-M e AlexNet) aplicadas ao problema de detecção de anomalias visuais.
    \item \textbf{Experimento 2 – Ajuste de Hiperparâmetros}: investigação do impacto de diferentes combinações de \textit{learning rate} e \textit{batch size} nos modelos mais promissores identificados no Experimento 1.
    \item \textbf{Experimento 3 – Estratégias de Generalização e Fine-Tuning}: aplicação de técnicas de aumento de dados, regularização e \textit{fine-tuning} para otimizar o modelo final e reduzir problemas de sobreajuste.
\end{itemize}

Cada experimento é detalhado nas subseções seguintes, incluindo seus objetivos, configurações, procedimentos adotados e resultados esperados.

% ----------------------------------------------------------
% EXPERIMENTO 1
% ----------------------------------------------------------

\section{Experimento 1 – Comparação de Arquiteturas CNN}
\label{sec:experimento1}

O primeiro experimento teve como objetivo comparar o desempenho de diferentes arquiteturas de Redes Neurais Convolucionais (CNNs) na tarefa de detecção automática de anomalias visuais em jogos digitais. Foram avaliados seis modelos amplamente utilizados em visão computacional: \textbf{VGG16}, \textbf{VGG19}, \textbf{ResNet50}, \textbf{DenseNet121}, \textbf{EfficientNetV2M} e \textbf{AlexNet}.

\subsection{Configuração Experimental}

Cada modelo foi ajustado para classificação binária (\textit{normal} e \textit{anômala}) com base em imagens do conjunto de dados preparado na Seção~\ref{chap:desenvolvimento}. O treinamento foi realizado com as seguintes configurações:

\begin{itemize}
    \item \textbf{Otimizador}: Adam;
    \item \textbf{Função de perda}: Binary Cross-Entropy;
    \item \textbf{Taxa de aprendizado}: 0{,}001;
    \item \textbf{Batch size}: 32;
    \item \textbf{Número de épocas}: 30;
    \item \textbf{Proporção de divisão}: 80\% treino e 20\% validação;
    \item \textbf{Data augmentation}: rotação de ±15°, brilho entre 0{,}8 e 1{,}2, e espelhamento horizontal.
\end{itemize}

O processo foi implementado utilizando a biblioteca \texttt{TensorFlow/Keras} e executado em ambiente GPU. Para cada arquitetura testada, foram gerados automaticamente os seguintes gráficos: evolução da acurácia e perda, matriz de confusão, métricas por classe (precisão, revogação e F1-score) e curva ROC.

\subsection{Resultados Obtidos}

A seguir, são apresentados os gráficos que ilustram o desempenho de cada modelo durante o treinamento e validação.

\subsubsection*{VGG16}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Experimento1_images/VGG16_curvas.png}
    \caption{Evolução da acurácia e perda da VGG16 durante o treinamento.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/Experimento1_images/VGG16_confusion_matrix.png}
    \caption{Matriz de confusão da VGG16.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Experimento1_images/VGG16_metrics_f1-score.png}
    \caption{F1-score por classe da VGG16.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/Experimento1_images/VGG16_roc_curve.png}
    \caption{Curva ROC da VGG16.}
\end{figure}

\subsubsection*{VGG19}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Experimento1_images/VGG19_curvas.png}
    \caption{Evolução da acurácia e perda da VGG19 durante o treinamento.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/Experimento1_images/VGG19_confusion_matrix.png}
    \caption{Matriz de confusão da VGG19.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Experimento1_images/VGG19_metrics_f1-score.png}
    \caption{F1-score por classe da VGG19.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/Experimento1_images/VGG19_roc_curve.png}
    \caption{Curva ROC da VGG19.}
\end{figure}

\subsubsection*{DenseNet121}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Experimento1_images/DenseNet121_curvas.png}
    \caption{Evolução da acurácia e perda da DenseNet121 durante o treinamento.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/Experimento1_images/DenseNet121_confusion_matrix.png}
    \caption{Matriz de confusão da DenseNet121.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Experimento1_images/DenseNet121_metrics_f1-score.png}
    \caption{F1-score por classe da DenseNet121.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{images/Experimento1_images/DenseNet121_roc_curve.png}
    \caption{Curva ROC da DenseNet121.}
\end{figure}

\subsubsection*{AlexNet, ResNet50 e EfficientNetV2M}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Experimento1_images/AlexNet_curvas.png}
    \caption{Evolução da acurácia e perda da AlexNet.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Experimento1_images/ResNet50_curvas.png}
    \caption{Evolução da acurácia e perda da ResNet50.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Experimento1_images/EfficientNetV2M_curvas.png}
    \caption{Evolução da acurácia e perda da EfficientNetV2M.}
\end{figure}

\subsection{Desempenho Quantitativo}

A Tabela~\ref{tab:exp1_resultados} resume os principais resultados de validação obtidos por cada arquitetura.

\begin{table}[H]
\centering
\caption{Resultados de validação dos modelos testados no Experimento 1.}
\label{tab:exp1_resultados}
\begin{tabular}{lcccc}
\toprule
\textbf{Modelo} & \textbf{Acurácia} & \textbf{Precisão} & \textbf{Recall} & \textbf{F1-score} \\
\midrule
VGG19            & 0{,}6607 & 0{,}6697 & 0{,}6607 & \textbf{0{,}6562} \\
DenseNet121      & 0{,}6525 & \textbf{0{,}6731} & 0{,}6525 & 0{,}6418 \\
VGG16            & 0{,}6410 & 0{,}6437 & 0{,}6410 & 0{,}6393 \\
AlexNet          & 0{,}5020 & 0{,}5914 & 0{,}5020 & 0{,}3408 \\
ResNet50         & 0{,}5000 & 0{,}2500 & 0{,}5000 & 0{,}3333 \\
EfficientNetV2M  & 0{,}5000 & 0{,}2500 & 0{,}5000 & 0{,}3333 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Resumo Técnico}

Os modelos \textbf{VGG19}, \textbf{DenseNet121} e \textbf{VGG16} apresentaram os melhores desempenhos gerais, com acurácia entre 64\% e 66\% e F1-score acima de 0{,}63. O modelo \textbf{VGG19} destacou-se por apresentar maior equilíbrio entre precisão e revogação. Por outro lado, \textbf{ResNet50} e \textbf{EfficientNetV2M} não demonstraram aprendizado efetivo, mantendo acurácia próxima a 50\%, o que indica possível falha na adaptação ao problema binário. O modelo \textbf{AlexNet}, embora simples, teve desempenho intermediário.

A análise detalhada desses resultados será discutida no Capítulo~\ref{chap:resultados}.
